<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TemCap – Project Page</title>
  <link rel="icon" type="image/png" href="./assets/images/Browser_Icon.png" />
  <!-- Tailwind (via CDN) -->
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-white text-gray-800">

  <!-- ==========  MOBILE‑ONLY COLLAPSIBLE MENU  ========== -->
  <details class="md:hidden bg-white border-b border-gray-200 shadow-sm mb-2">
    <summary class="px-4 py-3 font-semibold cursor-pointer select-none">
      Menu
    </summary>
    <div class="px-4 pb-5 text-sm space-y-6 leading-relaxed">
      <nav class="space-y-1">
        <a href="#abstract"   class="block hover:underline text-gray-700">Abstract</a>
        <a href="#method"     class="block hover:underline text-gray-700">Method</a>
        <a href="#contributions"    class="block hover:underline text-gray-700">Contributions</a>
        <a href="#metrics"    class="block hover:underline text-gray-700">Metrics</a>
        <a href="#impact"    class="block hover:underline text-gray-700">Impact</a>
        <a href="#resources"  class="block hover:underline text-gray-700">Resources</a>
        <a href="#acks"       class="block hover:underline text-gray-700">Acknowledgments</a>
        <a href="#citation"   class="block hover:underline text-gray-700">Citation</a>
        <a href="#contact"    class="block hover:underline text-gray-700">Contact</a>
      </nav>
      <hr>
      <!-- Paper Links -->
      <div>
        <h3 class="font-semibold mb-1">Paper Links</h3>
        <ul class="list-disc list-inside space-y-1">
          <li><a href="https://arxiv.org/pdf/2505.16594" class="text-blue-600 hover:underline">PDF (5.35 MB)</a></li>
          <li><a href="https://arxiv.org/abs/2505.16594" class="text-blue-600 hover:underline">arXiv page</a></li>
        </ul>
      </div>
    </div>
  </details>

  <!-- ==========  PAGE WRAPPER  ========== -->
  <div class="flex h-screen overflow-hidden">

    <!-- SIDEBAR — visible ≥ md only -->
    <aside class="hidden md:sticky md:top-0 md:block md:w-60 md:flex-shrink-0 bg-white border-r border-gray-200 p-6 overflow-y-auto">
      <h2 class="text-xl font-bold mb-4">Navigation</h2>
      <nav class="space-y-2 mb-6 text-sm leading-relaxed">
        <a href="#abstract"   class="block hover:underline text-gray-700">Abstract</a>
        <a href="#method"     class="block hover:underline text-gray-700">Method</a>
        <a href="#contributions"    class="block hover:underline text-gray-700">Contributions</a>
        <a href="#metrics"    class="block hover:underline text-gray-700">Metrics</a>
        <a href="#impact"    class="block hover:underline text-gray-700">Impact</a>
        <a href="#resources"  class="block hover:underline text-gray-700">Resources</a>
        <a href="#acks"       class="block hover:underline text-gray-700">Acknowledgments</a>
        <a href="#citation"   class="block hover:underline text-gray-700">Citation</a>
        <a href="#contact"    class="block hover:underline text-gray-700">Contact</a>
      </nav>

      <hr class="my-6">
      <h3 class="font-semibold mb-2 text-sm">Paper Links</h3>
      <ul class="list-disc list-inside space-y-1 text-sm leading-relaxed mb-6">
        <li><a href="https://arxiv.org/pdf/2505.16594" class="text-blue-600 hover:underline">PDF (5.35 MB)</a></li>
        <li><a href="https://arxiv.org/abs/2505.16594" class="text-blue-600 hover:underline">arXiv page</a></li>
      </ul>
      <hr class="my-6">
    </aside>

    <!-- MAIN CONTENT -->
    <main class="flex-1 overflow-y-auto p-6 md:p-8 space-y-20" id="main">

      <!-- HERO -->
      <header class="text-center mb-12">
        <h1 class="text-4xl font-extrabold leading-tight">
          Temporal Object Captioning for Street Scene Videos from LiDAR Tracks
        </h1>

        <!-- AUTHORS -->
        <p class="text-sm text-gray-500 mt-3">
          <a href="https://osvia.github.io/team.html"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Vignesh Gopinathan</a><sup>1,2</sup> •
          <a href="mailto:urs.zimmermann[at]aptiv.com"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Urs Zimmermann</a><sup>2</sup> •
          <a href="mailto:michael.arnold[at]aptiv.com"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Michael Arnold</a><sup>2</sup> •
          <a href="https://osvia.github.io/team.html"
            class="text-blue-600 hover:underline" target="_blank"
            rel="noopener noreferrer">Matthias Rottmann</a><sup>3</sup>
        </p>

        <!-- AFFILIATION FOOTNOTE -->
        <p class="text-xs text-gray-500 mt-1">
          <sup>1</sup> Department&nbsp;of&nbsp;Mathematics, University&nbsp;of&nbsp;Wuppertal, Wuppertal, Germany
          <br>
          <sup>2</sup> Aptiv, Germany
          <br>
          <sup>3</sup> Institute of Computer Science, Osnabrück University, Osnabrück, Germany
          <br><br>
          Winter Conference on Applications of Computer Vision (WACV), 2026
        </p>

        <!-- LOGO STRIP -->
        <div class="flex flex-wrap justify-center items-center gap-6 mt-4">
          <!-- OSVIA Lab Logo -->
          <a href="https://osvia.github.io/" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/KeyVisual_Dark.png" alt="OSVIA Lab Logo" class="h-full w-auto" loading="lazy">
          </a>

          <!-- University of Wuppertal Logo (example) -->
          <a href="https://www.uni-wuppertal.de/" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/BUW_Logo.jpg" alt="University of Wuppertal Logo" class="h-full w-auto" loading="lazy">
          </a>

          <!-- University of Osnabrück -->
          <a href="https://www.uni-osnabrueck.de/en" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/logo_osnabrück.jpg" alt="University of Osnabrück" class="h-full w-auto" loading="lazy">
          </a>

          <!-- Aptiv -->
          <a href="https://www.aptiv.com/" target="_blank" rel="noopener noreferrer" class="block h-10">
            <img src="./assets/images/aptiv_logo_color_rgb.png" alt="Aptiv" class="h-full w-auto" loading="lazy">
          </a>

          <!-- Add additional logos here (duplicate & adapt) -->
        </div>

      
        <!-- ACTION BUTTONS -->
        <div class="flex flex-wrap justify-center gap-3 mt-6">

          <!-- Paper / Document icon -->
          <a href="https://arxiv.org/abs/2505.16594"
            class="inline-flex items-center px-6 py-3 rounded-xl bg-blue-600 text-white font-semibold hover:bg-blue-700">
            <!-- Heroicons outline: DocumentText -->
            <svg class="w-5 h-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor"
                stroke-width="1.5" aria-hidden="true">
              <path stroke-linecap="round" stroke-linejoin="round"
                    d="M6 2.25h7.5L18 6.75v14.25a.75.75 0 01-.75.75H6.75A.75.75 0 016 21V2.25z" />
              <path stroke-linecap="round" stroke-linejoin="round"
                    d="M14.25 2.25v4.5h4.5" />
              <path stroke-linecap="round" stroke-linejoin="round"
                    d="M8.25 9.75h7.5m-7.5 3h7.5m-7.5 3h4.5" />
            </svg>
            arXiv Page
          </a>
        </div>


        <!-- Title Images Row (unchanged) -->
        <div class="flex flex-wrap justify-center gap-2 mt-6">
          <img src="./assets/images/figure1.png"  alt="Title image 1"
              class="h-64 w-auto max-w-[100%] md:max-w-none rounded-lg shadow-sm">
        </div>
      </header>
      <!-- /HERO -->


      <!-- ABSTRACT -->
      <section id="abstract" class="space-y-4 lg:max-w-6xl  lg:mx-auto">
        <h2 class="text-2xl font-semibold">Abstract</h2>
        <p>
          We introduce a fully automated LiDAR‑based captioning pipeline that produces object‑level, temporally grounded descriptions of street‑scene videos directly from object tracks. The captions encode lane position, motion relative to the host vehicle, and temporal transitions (e.g., lane changes). Such captions enable small and efficient video captioners like SwinBERT, trained on only the front‑camera frames to learn richer temporal semantics. Across three datasets—our proprietary data, Waymo, and NuScenes—models trained with our captions demonstrate improved temporal reasoning and reduced dependence on static background cues. We quantify this reduction using our proposed Visual Bias Measure (VBM), which evaluates a model’s reliance on scene appearance versus motion-driven dynamics.
        </p>
      </section>

      <!-- METHOD OVERVIEW -->
      <section id="method" class="space-y-6 bg-gray-100 rounded-xl px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Method Overview</h2>
        <p>
          Our captioning pipeline automatically converts LiDAR object tracks into rich, temporally grounded descriptions without any human annotation. The system uses a set of template sentences with placeholders that are filled using three types of attributes computed from LiDAR over time: lane position, motion relative to the host vehicle, and object type.
          <br><br>
          The procedure runs in two stages:
          <br><br>
          <b>A. Host-vehicle captioning:</b>
          The host vehicle’s velocity and yaw-rate signals are analyzed to segment the video into short clips, each representing a single host action (e.g., “decelerating while steering left”). These segments receive a simple template-based host caption that provides context for interpreting other objects’ behaviors.
          <br><br>
          <b>B. Neighbor (object) captioning:</b>
          For each segment, LiDAR-based 3D detection and tracking models yield trajectories for surrounding agents. Only moving objects within a defined region in front of the host vehicle and visible to the front camera are retained. At each timestamp, objects are assigned:
          <br>
          • a lane tag (left, right, host, oncoming, left/right-lateral),
          <br>
          • a motion tag (approaching, moving away, constant distance, stationary), and
          <br>
          • an object tag (car, truck, bike, pedestrian).
          <br><br>
          These tags form a time series that is compressed into a sequence of behavioral changes. Each unique state generates one sentence, enabling captions that naturally express transitions such as lane changes or shifts in motion. The first sentence introduces the object and the subsequent sentences describe how its behavior evolves over time.
          <br><br>
          The combinatorial template system supports thousands of distinct captions, allowing scalable generation of detailed, multi-sentence object-level descriptions directly from raw LiDAR data.
        </p>
        <div class="grid md:grid-cols-1 gap-x-6 gap-y-8">
          <!-- Column 1: nd‑to‑end data → caption → training pipeline -->
          <div class="space-y-3">
            <h3 class="text-xl font-semibold mb-3 text-center">Video captioning model</h3>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/method_image0.png" 
                  alt="method_image0" 
                  loading="lazy" 
                  class="w-full h-auto object-cover">
            </figure>
          </div>
          <p>
            The LiDAR-generated captions are used to train SwinBERT, a compact video-captioning transformer that operates solely on front-camera RGB frames. Training follows the masked-language modeling (MLM) strategy from the original SwinBERT design, with the model learning to predict the LiDAR-derived object-level captions from uniformly sampled frames per video clip.
          </p>

          <!-- Column 2: LiDAR‑to‑mask projection (convex hull) -->
          <div class="space-y-3">
            <h3 class="text-xl font-semibold mb-3 text-center">Object Mask generation (optional step)</h3>
            <figure class="rounded-lg overflow-hidden shadow-sm max-w-xl mx-auto">
              <img src="./assets/images/method_image1.png" 
                  alt="method_image1" 
                  loading="lazy" 
                  class="w-full h-auto object-cover">
            </figure>
          </div>
          <p>
            To further reduce reliance on background appearance and strengthen temporal grounding, the pipeline introduces an automated LiDAR-based object masking step. For each tracked object:
            <br><br>
            <b>A. 3D point selection:</b> LiDAR points inside the object’s 3D bounding box are extracted.
            <br><b>B. Projection:</b> These points are projected into the front-camera image using calibrated LiDAR-to-camera transforms.
            <br><b>C. Mask construction:</b> A tight convex hull is computed over the projected points (via the monotone-chain algorithm) and rasterized to create a clean object-specific pixel mask.
            <br><br>
            The resulting masks highlight the exact object referenced by the caption and suppress irrelevant background context, improving the model’s ability to learn dynamics rather than visual bias.
          </p>
        </div>
      </section>

      <!-- Key Contributions and Results -->
      <section id="contributions" class="space-y-6 bg-gray-100 rounded-xl px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Key Contributions and Results</h2>
        <p>
          • Scalable, generic captioning from LiDAR for temporally rich, object‑level descriptions without human labels.
          <br>• Temporal understanding beats strong baselines on captioning and retrieval across seen and unseen datasets.
          <br>• Visual Bias Measure (VBM), simple BLEU‑based metric, that quantifies reliance on static visual similarity. Training with our temporal captions consistently lowers VBM.
          <br>• Mask‑augmented training further improves generalization by directing attention to the object of interest, better temporal grounding between caption and the associated object.

        </p>
      </section>
      
      <!-- METRICS -->
      
      <section id="metrics" class="space-y-6 rounded-xl px-6 py-8 lg:max-w-6xl lg:mx-auto">
          <h2 class="text-2xl font-semibold">Metrics at a Glance</h2>
          <!-- ──  Captioning Quality ───────────────────────────────────── -->
          <div
          class="flex flex-col min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8">
          <!-- Explanatory text -->
          <div class="space-y-3 mb-4 min-[1360px]:mb-0 min-[1360px]:w-1/3">
              <h3 class="font-semibold text-lg">
                Captioning Quality (BLEU4 / CIDEr / SPICE)
              </h3>
              <p class="text-sm leading-relaxed">
                Trained solely on single-sentence captions from our proprietary dataset, the model generalizes well to both Waymo and NuScenes. It outperforms InternVideo, ViCLIP, and CLIP, even though those baselines face the easier task of selecting from predefined candidate captions.
              </p>
          </div>
      
          <!-- Chart -->
          <div class="flex-1 overflow-x-auto">
              <img src="./assets/images/table1.png" alt="table1" class="chart-image w-full border rounded-md" />
          </div>
          </div>
      
        <!-- ── Visual Bias Measure (VBM) ─────────────────────────────────── -->
          <div
              class="flex flex-col min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8">
              <!-- Explanatory text -->
              <div
                  class="space-y-3 mb-4 min-[1360px]:mb-0 min-[1360px]:w-1/3">
                  <h3 class="font-semibold text-lg">
                    Visual Bias Measure (VBM)
                  </h3>
                  <p class="text-sm leading-relaxed">
                    VBM captures how much retrieval performance drops, measured using BLEU4, when visually similar videos are removed from the candidate 
                    set in a video-to-video retrieval task. A <b>lower VBM</b> indicates that the model is less dependent on static scene appearance and relies 
                    more on true temporal cues. Our training achieves the <b>lowest VBM across all datasets</b>, and adding mask-based augmentation further 
                    reduces visual bias, lowering VBM by roughly <b>5 percentage points</b> on Waymo and <b>2 percentage points</b> on NuScenes.
                  </p>
              </div>

              <!-- Chart -->
              <div class="flex-1 overflow-x-auto">
                  <img src="./assets/images/table2.png" alt="table2" class="chart-image w-full border rounded-md"/>
              </div>
          </div>

        <!-- ── Zero‑Shot Generalization to Longer Captions ─────────────────────────────────── -->
        <div
        class="flex flex-col min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8">
        <!-- Explanatory text -->
        <div
            class="space-y-3 mb-4 min-[1360px]:mb-0 min-[1360px]:w-1/3">
            <h3 class="font-semibold text-lg">
              Zero‑Shot Generalization to Longer Captions
            </h3>
            <p class="text-sm leading-relaxed">
              Despite training only on single‑sentence captions, our embeddings retrieve videos 
              for <b>two‑sentence</b> captions (more complex maneuvers) with a <b>~20% higher</b> mean BLEU4 than the best baseline.
            </p>
        </div>

        <!-- Chart -->
        <div class="flex-1 overflow-x-auto">
            <img src="./assets/images/table3.png" alt="table3" class="chart-image w-full border rounded-md"/>
        </div>
    </div>

      </section>

      <!-- Broader Impact & Potential -->
      <section id="impact" class="space-y-4 lg:max-w-6xl  lg:mx-auto">
        <h2 class="text-2xl font-semibold">Broader Impact & Potential</h2>
        <p>
          • <b>Scaling data generation:</b> The trained video captioning model can be used to generate temporal object captions for camera-only dataset. Additionally, any LiDAR‑equipped dataset can be converted into rich temporal captioning dataset at low cost using out pseudo-labeling method.  
          <br>
          • <b>Beyond captioning:</b> The learned embeddings improve <b>video retrieval</b> by maneuvers and can aid <b>trajectory forecasting, risk reasoning, or behavior search</b> tools. 
          <br>
          • <b>Temporal understanding metric:</b> VBM, a metric to quantitaively evaluate temporal understanding capabilities of models. Lower VBM suggests robustness to background changes—useful for being agnostic to geographic and seasonal shifts.
        </p>
      </section>

      <!-- RESOURCES -->
      <section id="resources" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Resources & Links</h2>
        <ul class="list-disc list-inside space-y-2">
          <li><a href="https://arxiv.org/pdf/2505.16594" class="text-blue-600 hover:underline">PDF (5.35 MB)</a></li>
          <li><a href="https://arxiv.org/abs/2505.16594" class="text-blue-600 hover:underline">arXiv abstract &amp; BibTeX</a></li>
          <li><a href="#" class="text-blue-600 hover:underline">Code (coming soon)</a></li>
        </ul>
      </section>

      <!-- ACKNOWLEDGMENTS -->
      <section id="acks" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Acknowledgments</h2>
        <div class="mt-4 w-40 flex flex-col items-center">
          <a href="https://osvia.github.io/" target="_blank" rel="noopener noreferrer">
            <img src="./assets/images/KeyVisual_Dark.png" alt="OSVIA Lab Logo" loading="lazy" class="w-full h-auto">
          </a>
          <p class="mt-2 text-sm text-gray-600 text-center">
            <a href="https://osvia.github.io/" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer">OSVIA Lab</a>
          </p>
        </div>
      </section>

      <!-- CITATION (bottom) -->
      <section id="citation" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Citation</h2>

        <div class="relative group">
          <!-- BibTeX code block -->
          <pre id="bibtex-bottom"
                class="whitespace-pre overflow-x-auto
                      bg-gray-100 rounded-lg p-3
                      text-[0.70rem] leading-5 font-mono
                      text-gray-700 select-all">
                      @inproceedings{temporal-object-captioning-wacv2026,
                        title={Temporal Object Captioning for Street Scene Videos from LiDAR Tracks},
                        author={To be updated post-review},
                        booktitle={WACV},
                        year={2026},
                        note={Under review; project page}}
          </pre>

          <!-- copy‑to‑clipboard button -->
          <button
            type="button"
            aria-label="Copy BibTeX"
            class="absolute top-2 right-2
                    flex items-center gap-1
                    px-2 py-1 rounded-md text-xs font-medium
                    bg-gray-200 hover:bg-gray-300
                    transition
                    group-hover:opacity-100 opacity-0"
            onclick="
              navigator.clipboard
                .writeText(document.getElementById('bibtex-bottom').innerText)
                .then(btn=>{
                  this.textContent='✔︎ Copied';
                  setTimeout(()=>this.textContent='Copy',1500);
                });
            ">
            Copy
          </button>
        </div>
      </section>
      <!-- /CITATION (bottom) -->
      

      <!-- CONTACT -->
      <section id="contact" class="space-y-6 px-6 py-8 lg:max-w-6xl lg:mx-auto">
        <h2 class="text-2xl font-semibold">Contact</h2>
        <p class="text-sm">Have questions or want to collaborate? Reach out:</p>
        <ul class="space-y-1 text-sm">
          <li>Email: <a href="mailto:gopinathan[at]uni-wuppertal.de" class="text-blue-600 hover:underline">gopinathan[at]uni-wuppertal.de</a></li>
        </ul>
      </section>

      <!-- FOOTER -->
      <footer class="pt-12 pb-6 text-center text-xs text-gray-500">
        © 2025 From Label Error Detection to Correction • MIT License • Built using the 
        <a href="https://github.com/mmistakes/minimal-mistakes" target="_blank" class="text-blue-600 hover:underline">
          Minimal Mistakes layout template
        </a> • Site built by
        <a href="https://osvia.github.io/team.html" target="_blank" class="text-blue-600 hover:underline">
          Marco Schumacher
        </a>
      </footer>

    </main>
  </div>
  <!-- ==========  /PAGE WRAPPER  ========== -->
</body>
</html>